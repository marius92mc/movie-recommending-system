{
  "name": "movie-recommending-system",
  "tagline": "Movie Recommending System built on Spark, using Flask and ReactJS",
  "body": "# Movie Recommending System using Spark\r\n\r\nA web platform for recommending movies to users that \r\nlog in with their Facebook account. They rate movies and receive recommendations based \r\non their activity. They also receive a predicted rating for particular movies. \r\n\r\nTechnologies used: \r\n\r\n- Apache Spark\r\n- Flask - Python based web framework \r\n- PostgreSQL\r\n- React\r\n- jQuery, Ajax\r\n- Bootstrap\r\n\r\n\r\n## Requirements and installation\r\n\r\nIt is highly recommended to use <a href=\"http://docs.python-guide.org/en/latest/dev/virtualenvs/\" target=\"_blank\">virtualenv</a>. \r\n\r\nPlease see requirements.txt.\r\nTo install these packages, use the following command in a <a href=\"http://docs.python-guide.org/en/latest/dev/virtualenvs/\" target=\"_blank\"> virtualenv</a>.\r\n\r\n    $ pip install -r requirements.txt\r\n\r\nDownload Spark v1.6.1 from <a href=\"http://spark.apache.org/downloads.html\"> here</a>.\r\n\r\n    $ tar -xf <name_of_spark_archive>.tar\r\n\r\nFollow instructions from spark-1.6.1/README.md to build and install.\r\n\r\nEnvironment variables in your ~/.bash_profile for OS X or ~/.bashrc for Linux.\r\n\r\n    export SPARK_HOME=~/path/to/spark-1.6.1\r\n    export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\r\n    export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/pyhon/lib/py4j-0.9-src.zip:$PYTHONPATH\r\n    \r\nVerify it was successfully installed by running \r\n\r\n    $ ./bin/pyspark\r\n\r\nfrom spark-1.6.1/\r\n\r\nCopy <i>spark-env.sh</i>, <i>spark-defaults.conf</i> and <i>slaves</i> files \r\nfrom <i>spark_utils/</i> to <i>path/to/spark/conf/</i>. \r\n\r\n- <i>spark-env.sh</i> \r\n       - settings regarding the master node, like number of workers if manages, etc.\r\n         Note: Edit the SCALA_HOME, SPARK_WORKER_DIR environment variables \r\n         to your local needs. \r\n- <i>spark-defaults.conf</i> \r\n       - settings regarding the workers. \r\n          Any values specified as flags or in the properties file will be \r\n          passed on to the \tapplication and merged with those specified \r\n          through SparkConf invocation in the code. \r\n          Properties set directly on the SparkConf creation, in code, take highest precedence, \r\n          then flags passed to spark-submit or spark-shell, \r\n          then options in the spark-defaults.conf file.\r\n- <i>slaves</i> \r\n       - addresses of the workers.\r\n\r\nFor database we use PostgreSQL. \r\nDownload and install it from [here](http://www.postgresql.org/download/).\r\n\r\nAdd the following to ~/.bash_profile \r\n    \r\n    export PATH=$PATH:/Applications/Postgres.app/Contents/Versions/latest/bin \r\n\r\nto ~/.bash_profile. \r\n\r\n    $ which psql\r\n \r\nshould output like this\r\n    \r\n    /Applications/Postgres.app/Contents/Versions/latest/bin/psql\r\n\r\nCreate a PostgreSQL database\r\n\r\n    $ createdb rs\r\n    \r\nTo see the created database, got to Terminal and type \r\n\"psql\", then in the <i>postgres</i> console type \"\\l\", \r\nwhich will list all the databases available, \r\n\"\\connect rs;\" to connect to the newly created rs database created above, \r\nand \"\\d\" will show the available tables from the connected database.\r\n\r\nNote: When droping table from psql console, also drop the sequence mentioned in \r\nthe models, otherwise the autoincrementing id will not be reseted, as it should be. \r\n\r\n    DROP TABLE \"Users\"; DROP SEQUENCE seq_user_id; \r\n    DROP TABLE \"Movies\"; DROP SEQUENCE seq_movie_id;\r\n\r\nInstall npm packages needed for React\r\n\r\n    $ npm install\r\n\r\nAny changes on the React components from app/templates/client/components/ or \r\napp/templates/client/index.js file should be followed by the <i>build</i> command.\r\n\r\nBuild bundle.js from all the npm packages, scripts with behavior defined in package.json,  \r\n\r\n    $ npm run-script build\r\n\r\nor to build bundle.js and run the webpack server \r\n\r\n    $ npm run-script run \r\n    \r\nTo download the <b>dataset</b> that will be used, run the following script\r\n\r\n    $ python download_dataset.py\r\n\r\n## Usage\r\n- [ ] Write <b>full</b> instructions and an example. \r\n\r\nActivate the created virtualenv directory.\r\n\r\n    source name_of_virtualenv_directory/bin/activate\r\n\r\n### 1. Starting the Spark Master and Workers.\r\nFrom path/to/spark type\r\n    \r\n    $ sbin/start-master.sh\r\n    $ sbin/start-slaves.sh\r\n\r\nor \r\n\r\n    $ sbin/start-all.sh\r\n\r\nSimilar, to stop the Master and Workers. \r\n\r\n    $ sbin/stop-master.sh\r\n    $ sbin/stop-slaves.sh\r\n\r\nor \r\n\r\n    $ sbin/stop-all.sh\r\n\r\n\r\nSee Spark stats via UI at\r\n \r\n[http://localhost:8080](http://localhost:8080) \r\n\r\n[http://localhost:4040](http://localhost:4040)\r\n\r\n### 2. Sending the Python sources to Spark and run them\r\nRun from project directory, the following\r\n\r\n    $ sh path/to/spark/bin/spark-submit \\\r\n               --master spark://<server_name/server_ip>:7077 \\\r\n               --num-executors 2 \\\r\n               --total-executor-cores 2 \\\r\n               --executor-memory 2g \\\r\n               server.py [options] > stdout 2> stderr\r\n\r\nwhere server_name is yosemite/ubuntu/localhost if it's running locally. \r\n\r\nOptions, all <b>optional</b>, include:\r\n\r\n    --dataset <name>\r\n      Specify a dataset, e.g. \"ml-latest\" or \"ml-latest-small\". \r\n      If omitted, defaults to \"ml-latest\".\r\n\r\nLogs can be seen in the above provided files.\r\n\r\n    $ tail -f stdout\r\n    $ tail -f stderr\r\n\r\nBy default, as it is mentioned in <i>server.py</i>, CherryPy will use \r\nport 5434. \r\nChange it from the same file if it is busy.\r\n### 3. <b>Operations on the constructed model</b>\r\n\r\nGo to [http://0.0.0.0:5434/](http://0.0.0.0:5434/) and have <b>fun</b>. :fireworks:\r\n\r\nOr interact with the constructed ML model via terminal with the following commands.\r\n\r\n- <b>POSTing new ratings</b> to the model\r\n\r\n```\r\n$ curl --data-binary @user_ratings/user_ratings.file http://0.0.0.0:5434/<user_id>/ratings\r\n```\r\n\r\nwhere user_id is 0 by default representing a total new user, \r\noutside from those mentioned in the dataset.\r\n\r\n<b>Description</b>: POSTs user_id's ratings from <i>user_ratings.file</i>, where \r\nevery line has movie_id,rating. <br />\r\nWill start some computations and end up with an output representing \r\nthe ratings that has been submitted as a list of lists. <br />\r\nIn the server output window you will see the actual Spark computation \r\noutput together with CherryPy's output messages about HTTP requests.\r\n\r\nOutput represents ratings as - (user_id, movie_id, rating)\r\nrating awarded by the user from user_ratings.file.\r\n\r\n- <b>GETing best recommendations</b>\r\n\r\n```\r\n$ curl http://0.0.0.0:5434/<user_id>/ratings/top/<num_movies>\r\n```\r\n\r\nor in browser \r\n\r\nhttp://0.0.0.0:5434/user_id/ratings/top/num_movies\r\n\r\nExample\r\n\r\n    $ curl http://0.0.0.0:5434/0/ratings/top/10\r\n    $ curl http://0.0.0.0:5434/3/ratings/top/10\r\n\r\n[http://0.0.0.0:5434/0/ratings/top/10](http://0.0.0.0:5434/0/ratings/top/10)\r\n\r\n<b>Description</b>: Will present the best num_movies recommendations for user with user_id.\r\n\r\n- <b>GETing individual ratings</b>\r\n\r\n```\r\n$ curl http://0.0.0.0:5434/<user_id>/ratings/<movie_id>\r\n```\r\n\r\nor in browser\r\n\r\nhttp://0.0.0.0:5434/user_id/ratings/movie_id\r\n\r\nExample\r\n\r\n```\r\ncurl http://0.0.0.0:5434/0/ratings/500\r\ncurl http://0.0.0.0:5434/3/ratings/500\r\n```\r\n\r\n[http://0.0.0.0:5434/0/ratings/500](http://0.0.0.0:5434/0/ratings/500)\r\n\r\n[http://0.0.0.0:5434/1/ratings/500](http://0.0.0.0:5434/1/ratings/500)\r\n\r\n<b>Description</b>: Will get the predicted movie rating, from the model, of \r\nuser_id for movie_id. \r\n\r\n## Tests\r\nTODO\r\n\r\n## License\r\nTODO\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}